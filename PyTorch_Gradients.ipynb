{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Gradients.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xc4-uUqk7ne",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch Gradients\n",
        "This section covers the PyTorch <a href='https://pytorch.org/docs/stable/autograd.html'><strong><tt>autograd</tt></strong></a> implementation of gradient descent. Tools include:\n",
        "* <a href='https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward'><tt><strong>torch.autograd.backward()</strong></tt></a>\n",
        "* <a href='https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad'><tt><strong>torch.autograd.grad()</strong></tt></a>\n",
        "\n",
        "Before continuing in this section, be sure to watch the theory lectures to understand the following concepts:\n",
        "* Error functions (step and sigmoid)\n",
        "* One-hot encoding\n",
        "* Maximum likelihood\n",
        "* Cross entropy (including multi-class cross entropy)\n",
        "* Back propagation (backprop)\n",
        "\n",
        "<div class=\"alert alert-info\"><h3>Additional Resources:</h3>\n",
        "<strong>\n",
        "<a href='https://pytorch.org/docs/stable/notes/autograd.html'>PyTorch Notes:</a></strong>&nbsp;&nbsp;<font color=black>Autograd mechanics</font></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRe8ph59k_tr",
        "colab_type": "text"
      },
      "source": [
        "## Autograd - Automatic Differentiation\n",
        "In previous sections we created tensors and performed a variety of operations on them, but we did nothing to store the sequence of operations, or to apply the derivative of a completed function.\n",
        "\n",
        "In this section we'll introduce the concept of the <em>dynamic computational graph</em> which is comprised of all the <em>Tensor</em> objects in the network, as well as the <em>Functions</em> used to create them. Note that only the input Tensors we create ourselves will not have associated Function objects.\n",
        "\n",
        "The PyTorch <a href='https://pytorch.org/docs/stable/autograd.html'><strong><tt>autograd</tt></strong></a> package provides automatic differentiation for all operations on Tensors. This is because operations become attributes of the tensors themselves. When a Tensor's <tt>.requires_grad</tt> attribute is set to True, it starts to track all operations on it. When an operation finishes you can call <tt>.backward()</tt> and have all the gradients computed automatically. The gradient for a tensor will be accumulated into its <tt>.grad</tt> attribute.\n",
        "    \n",
        "Let's see this in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hAYqvp3lCIg",
        "colab_type": "text"
      },
      "source": [
        "## Back-propagation on one step\n",
        "We'll start by applying a single polynomial function $y = f(x)$ to tensor $x$. Then we'll backprop and print the gradient $\\frac {dy} {dx}$.\n",
        "\n",
        "$\\begin{split}Function:\\quad y &= 2x^4 + x^3 + 3x^2 + 5x + 1 \\\\\n",
        "Derivative:\\quad y' &= 8x^3 + 3x^2 + 6x + 5\\end{split}$\n",
        "\n",
        "#### Step 1. Perform standard imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhaBH0MolCdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7xOjuUxlWIl",
        "colab_type": "text"
      },
      "source": [
        "#### Step 2. Create a tensor with <tt>requires_grad</tt> set to True\n",
        "This sets up computational tracking on the tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsnvA6GVlACw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9r5G5BYlbfu",
        "colab_type": "text"
      },
      "source": [
        "#### Step 3. Define a function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ui3vw-Jlb0U",
        "colab_type": "code",
        "outputId": "9391497d-2dc1-4cf3-9fef-328fbb0adf70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(63., grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsxDiMHflmwY",
        "colab_type": "code",
        "outputId": "03f12adf-4420-4f91-ae5a-422d6f506ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY4dXbLfli0M",
        "colab_type": "text"
      },
      "source": [
        "Since $y$ was created as a result of an operation, it has an associated gradient function accessible as <tt>y.grad_fn</tt><br>\n",
        "The calculation of $y$ is done as:<br>\n",
        "\n",
        "$\\quad y=2(2)^4+(2)^3+3(2)^2+5(2)+1 = 32+8+12+10+1 = 63$\n",
        "\n",
        "This is the value of $y$ when $x=2$.\n",
        "\n",
        "#### Step 4. Backprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R7y_DI0ljJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.backward()           # this performs the backpropagation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_gva5gxlwnj",
        "colab_type": "text"
      },
      "source": [
        "#### Step 5. Display the resulting gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4HbHXhBlw_L",
        "colab_type": "code",
        "outputId": "f51dc079-3d7c-4831-d053-1dc7ac6ffff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x.grad)               # slope of the polynomial at that point, derivative of y wrt x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(93.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zqxjluclzv3",
        "colab_type": "text"
      },
      "source": [
        "Note that <tt>x.grad</tt> is an attribute of tensor $x$, so we don't use parentheses. The computation is the result of<br>\n",
        "\n",
        "$\\quad y'=8(2)^3+3(2)^2+6(2)+5 = 64+12+12+5 = 93$\n",
        "\n",
        "This is the slope of the polynomial at the point $(2,63)$.\n",
        "\n",
        "## Back-propagation on multiple steps\n",
        "Now let's do something more complex, involving layers $y$ and $z$ between $x$ and our output layer $out$.\n",
        "#### 1. Create a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJtyMgZCl0FS",
        "colab_type": "code",
        "outputId": "dab06648-17df-4279-d665-fdb315c73076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x = torch.tensor([[1.,2,3],[3,2,1]], requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [3., 2., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngOViHzPmtBm",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Create the first layer with $y = 3x+2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrlbBDzxmtZo",
        "colab_type": "code",
        "outputId": "0819489b-18df-4330-a8f2-16298fd32fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "y = 3*x + 2\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 5.,  8., 11.],\n",
            "        [11.,  8.,  5.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86gIhoLcm2pI",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Create the second layer with $z = 2y^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l--fh2Twm3J5",
        "colab_type": "code",
        "outputId": "7403408f-a043-4f05-d3de-e746b58d578e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "z = 2*y**2\n",
        "print(z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 50., 128., 242.],\n",
            "        [242., 128.,  50.]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgO8C_OLm8Os",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Set the output to be the matrix mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJPbRuoTm8qF",
        "colab_type": "code",
        "outputId": "fd0f0fff-3ee8-4fa1-8c0e-05cfbea98b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "out = z.mean()\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(140., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObFtvbbfnBi2",
        "colab_type": "text"
      },
      "source": [
        "#### 5. Now perform back-propagation to find the gradient of x w.r.t out\n",
        "(If you haven't seen it before, w.r.t. is an abbreviation of <em>with respect to</em>)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqpPYQ2CnCAC",
        "colab_type": "code",
        "outputId": "68f085b2-7606-4c4d-f7ea-4b6770507204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "out.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[10., 16., 22.],\n",
            "        [22., 16., 10.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2srC9wVzogih",
        "colab_type": "text"
      },
      "source": [
        "You should see a 2x3 matrix. If we call the final <tt>out</tt> tensor \"$o$\", we can calculate the partial derivative of $o$ with respect to $x_i$ as follows:<br>\n",
        "\n",
        "$o = \\frac {1} {6}\\sum_{i=1}^{6} z_i$<br>\n",
        "\n",
        "$z_i = 2(y_i)^2 = 2(3x_i+2)^2$<br>\n",
        "\n",
        "To solve the derivative of $z_i$ we use the <a href='https://en.wikipedia.org/wiki/Chain_rule'>chain rule</a>, where the derivative of $f(g(x)) = f'(g(x))g'(x)$<br>\n",
        "\n",
        "In this case<br>\n",
        "\n",
        "$\\begin{split} f(g(x)) &= 2(g(x))^2, \\quad &f'(g(x)) = 4g(x) \\\\\n",
        "g(x) &= 3x+2, &g'(x) = 3 \\\\\n",
        "\\frac {dz} {dx} &= 4g(x)\\times 3 &= 12(3x+2) \\end{split}$\n",
        "\n",
        "Therefore,<br>\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i} = \\frac{1}{6}\\times 12(3x+2)$<br>\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = 2(3(1)+2) = 10$\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 2(3(2)+2) = 16$\n",
        "\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=3} = 2(3(3)+2) = 22$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOZeRqQXolp_",
        "colab_type": "text"
      },
      "source": [
        "## Turn off tracking\n",
        "There may be times when we don't want or need to track the computational history.\n",
        "\n",
        "You can reset a tensor's <tt>requires_grad</tt> attribute in-place using `.requires_grad_(True)` (or False) as needed.\n",
        "\n",
        "When performing evaluations, it's often helpful to wrap a set of operations in `with torch.no_grad():`\n",
        "\n",
        "A less-used method is to run `.detach()` on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor."
      ]
    }
  ]
}